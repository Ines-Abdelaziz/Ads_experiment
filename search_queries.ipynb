{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import csv\n",
    "import time  # Optional: to add delay between API calls if needed\n",
    "\n",
    "\n",
    "openai.api_key =\"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\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_queries_for_theory(theory, model=\"gpt-4\"):\n",
    "    prompt = (\n",
    "        f\"Generate 3 realistic Google search queries that support the following conspiracy theory. \"\n",
    "        f\"The queries should sound like ones an average believer would type—short, natural, and based on everyday language. \"\n",
    "        f\"Focus on phrasing people use when looking for confirming evidence, whistleblower claims, or hidden truths. \"\n",
    "        f\"Avoid overly formal or technical wording.\\n\\n\"\n",
    "        f\"Theory: {theory}\\n\"\n",
    "        f\"Return the queries as a simple list with no explanations.\"\n",
    "    )\n",
    "\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You generate realistic Google search queries for believers of conspiracy theories.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    return content.split(\"\\n\")\n",
    "\n",
    "def generate_queries_for_headline(headline, model=\"gpt-4\"):\n",
    "\n",
    "    prompt = (\n",
    "        f\"Given the following news headline, generate 3 realistic Google search queries that a typical user might type \"\n",
    "        f\"to factually learn more about the event or topic. The queries should be simple, natural, and focused on getting accurate information. \"\n",
    "        f\"Avoid conspiracy language or opinion-based phrasing.\\n\\n\"\n",
    "        f\"Headline: {headline}\\n\"\n",
    "        f\"Return the queries as a simple list with no explanations.\"\n",
    "    )\n",
    "\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are simulating a curious, average internet user who wants to get reliable, accurate, and up-to-date information about news headlines.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    return content.split(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 300 queries to 'conspiracy_queries.csv'.\n"
     ]
    }
   ],
   "source": [
    "def generate_all_and_save(theories, filename=\"conspiracy_queries.csv\"):\n",
    "    rows = []\n",
    "\n",
    "    for theory in theories:\n",
    "        try:\n",
    "            queries = generate_queries_for_theory(theory)\n",
    "            for query in queries:\n",
    "                cleaned_query = query.lstrip('-').strip()\n",
    "                rows.append({\"Theory\": theory, \"Search Query\": cleaned_query})\n",
    "            time.sleep(1)  # Optional: avoid rate limits\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing theory '{theory}': {e}\")\n",
    "\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        fieldnames = [\"Theory\", \"Search Query\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"Saved {len(rows)} queries to '{filename}'.\")\n",
    "\n",
    "\n",
    "# read the conspiracies from csv file\n",
    "\n",
    "conspiracies = []\n",
    "with open('conspiracies.csv', mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        conspiracies.append(row[0])  # Assuming the conspiracy theory is in the first column\n",
    "\n",
    "generate_all_and_save(conspiracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 317 queries to 'news_queries.csv'.\n"
     ]
    }
   ],
   "source": [
    "# get 15 random headlines from each topic from csv \n",
    "import random\n",
    "import pandas as pd\n",
    "# read 15 headlines from each csv file\n",
    "headlines = []\n",
    "for topic in ['Business', 'Science', 'World', 'Nation', 'Health', 'Technology', 'Politics']:\n",
    "    df = pd.read_csv(f'{topic}.csv')\n",
    "    random_headlines = df.sample(15)['description'].tolist()\n",
    "    headlines.extend(random_headlines)\n",
    "\n",
    "# generate queries for each headline\n",
    "def generate_all_headlines_and_save(headlines, filename=\"news_queries.csv\"):\n",
    "    rows = []\n",
    "\n",
    "    for headline in headlines:\n",
    "        try:\n",
    "            queries = generate_queries_for_headline(headline)\n",
    "            for query in queries:\n",
    "                cleaned_query = query.lstrip('-').strip()\n",
    "                rows.append({\"Headline\": headline, \"Search Query\": cleaned_query})\n",
    "            time.sleep(1)  # Optional: avoid rate limits\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing headline '{headline}': {e}\")\n",
    "\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        fieldnames = [\"Headline\", \"Search Query\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"Saved {len(rows)} queries to '{filename}'.\")\n",
    "generate_all_headlines_and_save(headlines)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50 cleaned queries to 'cleaned_queries.csv'\n"
     ]
    }
   ],
   "source": [
    "#READ THE CSV FILES\n",
    "import pandas as pd\n",
    "import re  # Import the re module for regular expressions\n",
    "# Read the conspiracy queries\n",
    "conspiracy_df = pd.read_csv('conspiracy_queries.csv')\n",
    "# Read the news queries\n",
    "news_df = pd.read_csv('news_queries.csv')\n",
    "\n",
    "grouped = conspiracy_df.groupby('Theory')['Search Query'].apply(list).reset_index()\n",
    "\n",
    "# Filter headlines with at least 2 queries\n",
    "eligible = grouped[grouped['Search Query'].apply(len) >= 2]\n",
    "\n",
    "# Randomly sample 25 headlines\n",
    "sampled = eligible.sample(n=25, random_state=31)\n",
    "\n",
    "# Collect 2 random queries from each headline\n",
    "queries = []\n",
    "for qlist in sampled['Search Query']:\n",
    "    chosen = random.sample(qlist, 2)\n",
    "    for query in chosen:\n",
    "        # Clean the query: remove leading number and quotes\n",
    "        cleaned = re.sub(r'^\\d+\\.\\s*[\"“”]?|[\"“”]$', '', query.strip())\n",
    "        queries.append(cleaned)\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "output_df = pd.DataFrame(queries, columns=['Query'])\n",
    "output_df.to_csv('misinfo2_queries.csv', index=False)\n",
    "\n",
    "print(\"Saved 50 cleaned queries to 'cleaned_queries.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "# Parameters\n",
    "num_users = 13\n",
    "batches_per_user = 15\n",
    "conspiracy_count = 537\n",
    "control_count = 379\n",
    "\n",
    "# Generate video ID lists\n",
    "conspiracy_videos = pd.read_csv('conspiracy_videos_final.csv')['url'].tolist()\n",
    "control_videos =pd.read_csv('control_videos_cleaned_final.csv')['video_id'].tolist()\n",
    "control_videos=[\"https://www.youtube.com/watch?v=\"+str(x) for x in control_videos]\n",
    "users =[\"Neutral\"]\n",
    "\n",
    "\n",
    "# Main batch generation\n",
    "for user in users:\n",
    "    user_folder = f\"./{user}\"\n",
    "    os.makedirs(user_folder, exist_ok=True)\n",
    "\n",
    "    for batch_num in range(1, batches_per_user + 1):\n",
    "        # Select 25 random conspiracy and 25 random control videos\n",
    "        batch = random.choices(conspiracy_videos, k=25) + random.choices(control_videos, k=25)\n",
    "        random.shuffle(batch)\n",
    "\n",
    "        # Save batch to CSV\n",
    "        batch_file = os.path.join(user_folder, f\"{batch_num}.csv\")\n",
    "        with open(batch_file, mode=\"w\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"video_order\", \"video_id\"])\n",
    "            for order, video_id in enumerate(batch, start=1):\n",
    "                writer.writerow([order, video_id])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
